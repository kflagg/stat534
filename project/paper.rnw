\documentclass[11pt]{article}
\usepackage{fullpage,graphicx,float,amsmath,enumitem,fancyhdr,hyperref}

\setlist{parsep=5.5pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5.5pt}

\usepackage[backend=bibtex,style=authoryear,citestyle=authoryear-comp]{biblatex}
\addbibresource{references.bib}

\newcommand\assignment{Stat 534 Project: \\
Extrapolation from Poisson Process Intensity Surface Models}
\newcommand\myname{Kenny Flagg}
\newcommand\duedate{\today}

\pagestyle{fancy}
\lhead{Stat 534 Project}
\chead{\duedate}
\rhead{\myname}
\setlength{\headheight}{18pt}
\setlength{\headsep}{2pt}

\title{\assignment}
\author{\myname}
\date{\duedate}

\begin{document}
\maketitle

<<setup, echo = FALSE, message = FALSE, cache = FALSE>>=
library(knitr)
library(extrafont)
opts_chunk$set(echo = FALSE, comment = NA, message = FALSE,
               fig.path = 'paper_figure/', cache.path = 'paper_cache/',
               show.signif.stars = FALSE,
               fig.align = 'center', fig.width = 6.5, fig.height = 2.5,
               fig.pos = 'H', size = 'footnotesize', dev = 'pdf',
               dev.args = list(family = 'CM Roman', pointsize = 11))
knit_theme$set('print')

library(xtable)
options(xtable.table.placement = 'H', width = 80, scipen = 2,
        xtable.sanitize.rownames.function = function(x){return(x)},
        show.signif.stars = FALSE)
@

<<packages, message = FALSE, cache = FALSE>>=
library(spatstat)

set.seed(83534)
@


\section{Introduction}

One little-studied application of inhomogeneous point process intensity
estimation is the use of an intensity surface estimated from events in a
subregion to predict the intensity over the entire region of interest. This
procedure would be relevant whenver it is known or suspected that some type of
plant, animal, or other item occurs in the region, and the goal of the
analysis is to map the trend in where these tend to be located rather than
estimate parameters of some process at work across hypothetical replicates of
similar regions. It may be prohibitively expensive or difficult to observe all
events over the entire region, so a sample of subregions is taken. For
example, conservationists want to study the spatial distribution of an
endangered plant across a large region of thick jungle so that they can
establish a preserve where the plant is protected. They cannot search the
entire jungle, so they take a simple random sample of quadrats and record the
locations of the plants in those quadrats. They will then want to fit a model
describing the trends in intensity of these plants extrapolated over the
entire jungle. In this setting, the objective is to map the realized spatial
intensity of the plant over this jungle so that these plants can be protected,
not to estimate parameters of the process that arranges plants of this species
at jungles like this one.

Another situation where it is useful to extrapolate point pattern intensity
outside of the observed region is in mapping subsurface geomagnetic anomalies,
such as the munitions debris found at former military test ranges. This is
frequently done in the early stages of an unexploded ordnance (UXO)
remediation, where the intensity surface of inert munitions fragments is
used to identify the locations of targets so that the search for UXO can be
focused on the sections of the site most likely to contain it. The anomalies
are only observed when detection equipment passes directly over them, but the
project leaders are concerened with finding UXO and do not want to waste
resources finding every inert fragment that could be found by metal detectors.
The most common data collection method is to take a systematic sample of
straight-line transects and observe anomalies in rectangular regions centered
along the transects. Frequently, moving averages of the intensity are computed
in circular windows and an intensity map is produced using ordinary kriging to
predict the intensity in a grid of these windows. There are several problems
with this approach: it assumes stationarity when the moving averages are
believed to be non-stationary, it ignores the point process nature of the
data, and it is very sensitive to the window size~\parencite{vspguide,wp}. In
this project, I evaluate the use of polynomial trend surface models for the
log-intensity at a simulated UXO site.


\section{Surface Fitting by Maximum Likelihood}

It is theoretically possible to use maximum likelihood or maximum
pseudo-likelihood methods to fit trend surface models or regression models
(with covariates) for the intensity of an inhomogeneous point process, but
these are not widely used because, in the most general cases, the problems
are ``notoriously intractable''~\parencite{digglebook}. However, for spatial
Poisson processes, trend surface models are tenable with the help of some
numerical methods.

The log-likelihood of an unmarked Poisson process on a region \(D\) with
intensity \(\lambda(\mathbf{s})\) is found by conditioning on the number of
events in the following manner. The number of events in \(D\) follows a
Poisson distribution with mean \(\mu=\int_{D}\lambda(\mathbf{s})d\mathbf{s}\).
Given that \(n\) events occured, their locations \(\mathbf{s}_{1},\dots,
\mathbf{s}_{n}\) are independent and identically distributed with density
\(\lambda(\mathbf{s})/\mu\). Then the log-likelihood of the intensity is
\begin{align*}
\ell(\lambda) &= \left\{-\mu + n\log(\mu) - \log(n!)\right\}
+ \sum_{i=1}^{n}\left\{\log\left(\lambda(\mathbf{s}_{i})\right)
- \log(\mu)\right\} \\
&= \sum_{i=1}^{n}\log\left(\lambda(\mathbf{s}_{i})\right)
-\int_{D}\lambda(\mathbf{s})d\mathbf{s} - \log(n!).
\end{align*}
A natural way to model the intensity is to define a log-linear model
\begin{equation*}
\log(\lambda(\mathbf{s})) = \mathbf{x}(\mathbf{s})^{T}\boldsymbol{\beta}
\end{equation*}
where \(\mathbf{x}(\mathbf{s})\) can, in principle, include functions of the
spatial coordinates and also covariates. Unfortunately, the likelihood becomes
\begin{equation*}
\sum_{i=1}^{n}\mathbf{x}(\mathbf{s})^{T}\boldsymbol{\beta}
-\int_{D}\exp\left(\mathbf{x}(\mathbf{s})^{T}
\boldsymbol{\beta}\right)d\mathbf{s} - \log(n!)
\end{equation*}
so \(\mathbf{x}(\mathbf{s})\) must be known (or predicted) either across the
entire region \(D\) or at enough locations to approximate the integral
numerically. Thus, covariates require some extra data collection or modeling
effort to incorporate, but trend surfaces are feasible.

\textcite{bermanturner} proposed a practical method for fitting these models
with generalized linear model (GLM) software using quadratic approximations to
the likelihood, even generalizing to other link functions besides the log link.
Their method is based on partitioning \(D\) such that each subset in the
partition contains at most one event, and then maximizing a weighted
pseudolikelihood based upon a binomial or Poission distribution for the count
of events in each subset. They comment that the covariance matrices produced
by standard GLM software under this method are reasonable approximations to
the correct covariance matrices that would be derived from the true likelihood
as long as the quadrature approximation of the likelihood converges to the
true likelihood as the resolution of the partition is increased.
This was followed up by \textcite{baddeleycoeurjolly14}, who developed a
logistic regression-based approach that avoids the quadratic approximation,
thereby reducing bias in the coefficient estimates. Both methods are available
in the \texttt{spatstat} package~\parencite{spatstat} for R~\parencite{baser}.
The \texttt{spatstat} function \texttt{ppm} implements these methods using the
base R \texttt{glm} function to maximize the likelihood; I will use the Berman
and Turner method because it is the default in \texttt{ppm} and therefore
better reflects the experience of a na\"{i}ve practitioner who is not an
expert on spatial point processes.


\section{Examples}

To illustrate model fitting and prediction, I present two examples before
proceeding to the simulation study. The first is a toy example where I
simulate from a Poisson process with a true log-linear intensity function and
then estimate coefficients using the true model form. The second example
introduces the my UXO site and provides a case study of fitting a polynomial
trend surface to one realization of the the site.


\section{True Log-Linear Intensity Surface}

<<hw4_sim>>=
simulate_win <- owin(c(0, 1), c(0, 1))
omit_win <- owin(c(0.6, 0.9), c(0.6, 0.9))
observe_win <- setminus.owin(simulate_win, omit_win)
predict_win <- owin(c(-0.5, 1.5), c(-0.5, 1.5))
hw4 <- rpoispp(function(x, y){exp(5 * x + 2 * y)}, win = simulate_win)
@

Previously this semester, we worked with an example of a Poisson process on
the unit square \(\{(x,y):\Sexpr{min(simulate_win$xrange)} < x
< \Sexpr{max(simulate_win$xrange)}, \Sexpr{min(simulate_win$yrange)} < y
< \Sexpr{max(simulate_win$yrange)}\}\) with the log-linear intensity surface
\begin{equation*}
\log(\lambda(x,y)) = 5x + 2y.
\end{equation*}
I will continue this example, fitting the model
\begin{equation*}
\log(\lambda(x,y)) = \beta_{0} + \beta_{1}x + \beta_{2}y
\end{equation*}
first using the process observed over the full region (the entire unit
square), and then using the process observed in a subregion treating
\(\{(x,y): \Sexpr{min(omit_win$xrange)} < x < \Sexpr{max(omit_win$xrange)},
\Sexpr{min(omit_win$yrange)} < y < \Sexpr{max(omit_win$yrange)}\}\) as
unobserved. This unobserved section is a region of relatively high intensity,
so if the estimation or prediction methods are sensitive to the observation
window I expect omitting this section to have a large effect.

I will use both estimated models to predict on
\(\{(x,y): \Sexpr{min(predict_win$xrange)} < x
< \Sexpr{max(predict_win$xrange)}, \Sexpr{min(predict_win$yrange)} < y
< \Sexpr{max(predict_win$yrange)}\}\). Figure~\ref{fig:hw4_plot} shows the
realization of this process that I will use for both model fits.

<<hw4_plot, fig.pos = 'b!', fig.cap = 'One realization of a Poisson process with log-linear intensity on the unit square. The dashed square shows the outline of the ``unobserved" region; when fitting the model to the subregion, the points inside it will be discarded and it will not be considered in constructing the approximation to the likelihood.'>>=
par(mar = c(0, 0, 1, 0))
plot(hw4, main = 'Event Locations')
plot(omit_win, lty = 2, add = TRUE)
@

For clarity, I denote the estimated models as
\begin{equation*}
\log\left(\widehat{\lambda}_{f}(x,y)\right) = \widehat{\beta}^{(f)}_{0}
+ \widehat{\beta}^{(f)}_{1}x + \widehat{\beta}^{(f)}_{2}y
\end{equation*}
for the model fit to the full region, and
\begin{equation*}
\log\left(\widehat{\lambda}_{s}(x,y)\right) = \widehat{\beta}^{(s)}_{0}
+ \widehat{\beta}^{(s)}_{1}x + \widehat{\beta}^{(s)}_{2}y
\end{equation*}
for the model fit to the subregion. I use \texttt{ppm} to fit the models
with its default likelihood approximation and isotropic edge correction.
The coefficitents and standard errors are similar for both estimated models,
and the true values \(\beta_{0} = 0\), \(\beta_{1} = 5\), and
\(\beta_{2} = 2\) all fall within one standard error of the
estimates~(Tables~\ref{hw4estf} and \ref{hw4ests}).

After estimating the models, I use \texttt{spatstat}'s \texttt{predict.ppm}
method to predict the intensity surface on a \(128 \times 128\) lattice of
locations in the region \(\{(x,y): \Sexpr{min(predict_win$xrange)} < x
< \Sexpr{max(predict_win$xrange)}, \Sexpr{min(predict_win$yrange)} < y
< \Sexpr{max(predict_win$yrange)}\}\)~(Figure~\ref{fig:hw4_fitplot}). As
expected from the similarity of the coefficient estimates, the predicted
log-intensity surfaces for both models show very similar linear trends~(top
row of the figure). At a glance the prediction standard errors for both
models appear similar to the predicted intensities, as seen in the middle
row of the figure~(shown on a logarithmic scale for visibility). In a linear
model setting we would expect the prediction standard error to increase with
the distance from the observations, but no such trend is apparent here; even
in the bottom left of the prediction region the standard errors decrease as
the predicted intensity decreases. However, it is illuminating to plot the
predictor's relative standard error (the ratio of the standard error to the
predicted intensity, bottom row of the figure). This ratio is not constant,
and in fact it is lowest where the highest intensity of events was observed.

In summary, the estimation procedure does an adequate job of estimating the
parameters of the true model for these data, and the standard error of the
predicted intensity appears to be more strongly related to the distance from
the observed events than to the distance from the observed region.

<<hw4_fit_full>>=
hw4_fit_f <- ppm(hw4 ~ x + y, correction = 'isotropic')
hw4_pred_f <- predict(hw4_fit_f, window = predict_win, se = TRUE)
# By default predictions are on a 128 by 128 grid.
@

<<hw4_fit_s>>=
hw4_fit_s <- ppm(hw4[observe_win] ~ x + y, correction = 'isotropic')
hw4_pred_s <- predict(hw4_fit_s, window = predict_win, se = TRUE)
@

<<hw4_fitcoefs_f, results = 'asis'>>=
hw4_coef_f <- summary(hw4_fit_f)$coefs.SE.CI[,c('Estimate', 'S.E.')]
rownames(hw4_coef_f) <- c('\\(\\widehat{\\beta}^{(f)}_{0}\\)',
                          '\\(\\widehat{\\beta}^{(f)}_{1}\\)',
                          '\\(\\widehat{\\beta}^{(f)}_{2}\\)')
print(xtable(hw4_coef_f, label = 'hw4estf',
             caption = 'Estimated coefficients for the log-linear trend model fit using the full region.'),
      table.placement = 't!')
@

<<hw4_fitcoefs_s, results = 'asis'>>=
hw4_coef_s <- summary(hw4_fit_s)$coefs.SE.CI[,c('Estimate', 'S.E.')]
rownames(hw4_coef_s) <- c('\\(\\widehat{\\beta}^{(s)}_{0}\\)',
                          '\\(\\widehat{\\beta}^{(s)}_{1}\\)',
                          '\\(\\widehat{\\beta}^{(s)}_{2}\\)')
print(xtable(hw4_coef_s, label = 'hw4ests',
             caption = 'Estimated coefficients for the log-linear trend model fit using the subregion.'),
      table.placement = 't!')
@

<<hw4_fitplot, fig.pos = 'p', fig.height = 8, fig.cap = 'Estimated log-intensity surface, log-scale prediction standard errors, and relative standard error from models fit using all events in the full simulation region (left) and events in a subset of the simulation region (right). The regions and event locations are overlaid in white.'>>=
par(mfrow = c(3, 2), mar = c(1, 0, 1.2, 2), las = 2, cex = 1)

plot(log(hw4_pred_f$estimate), zlim = c(-4.5, 10.5),
     main = expression(log(hat(lambda)[f](italic(list(x,y))))))
plot(simulate_win, add = TRUE, border = '#ffffff80')
points(hw4, pch = '.', col = 'white')

plot(log(hw4_pred_s$estimate), zlim = c(-4.5, 10.5),
     main = expression(log(hat(lambda)[s](italic(list(x,y))))))
plot(observe_win, add = TRUE, border = '#ffffff80')
points(hw4[observe_win], pch = '.', col = 'white')

plot(log(hw4_pred_f$se), zlim = c(-4.5, 10.5),
     main = expression(log(SE(hat(lambda)[f](italic(list(x,y)))))))
plot(simulate_win, add = TRUE, border = '#ffffff80')
points(hw4, pch = '.', col = 'white')

plot(log(hw4_pred_s$se), zlim = c(-4.5, 10.5),
     main = expression(log(SE(hat(lambda)[s](italic(list(x,y)))))))
plot(observe_win, add = TRUE, border = '#ffffff80')
points(hw4[observe_win], pch = '.', col = 'white')

plot(hw4_pred_f$se/hw4_pred_f$estimate, zlim = c(0, 1),
     main = expression(SE(hat(lambda)[f](italic(list(x,y)))) /
                         hat(lambda)[f](italic(list(x,y)))))
plot(simulate_win, add = TRUE, border = '#ffffff80')
points(hw4, pch = '.', col = 'white')

plot(hw4_pred_s$se/hw4_pred_s$estimate, zlim = c(0, 1),
     main = expression(SE(hat(lambda)[s](italic(list(x,y)))) /
                         hat(lambda)[s](italic(list(x,y)))))
plot(observe_win, add = TRUE, border = '#ffffff80')
points(hw4[observe_win], pch = '.', col = 'white')
@


\section{Simulated UXO Site}

<<uxoexampledata>>=
# Define the window for the UXO site.
site_window <- owin(poly = cbind(x = c(1564294, 1564495, 1556870, 1557126),
                                 y = c(535421, 541130, 541085, 535576)),
                    unitname = c('foot', 'feet'))
@

In my writing project, I evaluated ordinary kriging of moving average
intensity values on a simulated tank training range. I will use the same site
for this project. The site is a
\Sexpr{sprintf('%.2f', area(site_window) / 43560)} acre quadrilateral region
with homogeneous Poisson process producing background anomalies with an
intensity of 100 anomalies per acre, and two regions of concentrated munitions
use generated as Poisson processes with bivariate Gaussian intensity. The
total intesnsity at the center of each concentrated munitions use region is
300 anomalies per acre~(Figure~\ref{fig:truth}).

<<truth, cache = TRUE, fig.width = 4, fig.pos = 't', fig.cap = 'True intensity at the simulated UXO site.'>>=
# Bivariate normal intensity function that I originalally wrote for my
# writing project, parameterized in terms of the major and minor axes scales
# and the angle of the major axis from horizontal.
#   a is horizontal axis, b is vertical axis, r is rotation angle
gauss.elliptic <- function(x, y, mu.x = 0, mu.y = 0, s.a = 1, s.b = 1,
                           r = 0, maxrate = 1){
  rot <- zapsmall(matrix(c(cos(r), sin(r), -sin(r), cos(r)), nrow = 2))
  ab <- diag(c(s.a^2, s.b^2))
  sigma <- rot %*% ab %*% t(rot)
  siginv <- solve(sigma)
  mu <- matrix(c(mu.x, mu.y), nrow = 2)
  mat <- matrix(rbind(x, y), nrow = 2)
  return(maxrate * apply(mat, 2, function(vec){
      exp(-t(vec - mu) %*% siginv %*% (vec - mu) / 2)
    }))
}

# Create an image of the surface in 20 ft by 20 ft pixels.
x <- seq(1556880, 1564495, by = 20)
y <- seq(535431, 541130, by = 20)
intense.mat <- matrix(100, nrow = length(x), length(y))
for(i in seq_along(x)){
  for(j in seq_along(y)){
    intense.mat[i, j] <- 100 + gauss.elliptic(x[i], y[j],
        mu.x = 1558400, mu.y = 540000,
        s.a = 800 / (2 * qnorm(0.995)), s.b = 1200 / (2 * qnorm(0.995)),
        r = pi/6, maxrate = 200
      ) + gauss.elliptic(x[i], y[j],
        mu.x = 1562000, mu.y = 537000,
        s.a = 2000 / (2 * qnorm(0.995)), s.b = 900 / (2 * qnorm(0.995)),
        r = 0, maxrate = 200
      )
  }
}
intense.im <- im(t(intense.mat), x, y, unitname = c('foot', 'feet'))

par(mar = c(0, 0, 1, 3))
plot(intense.im, main = 'True Intensity Surface',
     border = NA, zlim = c(100, 300), las = 2)
plot(setminus.owin(owin(c(1556870, 1564495), c(535421, 541130),
                        unitname = c('foot', 'feet')),
     site_window), col = 'white', border = 'white', lwd = 2, add = TRUE)
mtext('Anomalies per Acre', 4, line = 2)
@

<<sim2000obs, fig.pos = 't', fig.cap = 'Observed geomagnetic anomalies from one realization of the UXO site. The transects are not shown on this plot because they would obscure the points.'>>=
# Load the sample data used in my writing project.
# The anomaly file contains the locations of the observed anamolies and the
# cog file contains the course-over-ground waypoints on the transects.
ex_anom <- read.csv('easy_sample_tTA2_p200_bg100_fg200_rep2000.anomaly')
ex_path <- read.csv('easy_sample_tTA2_p200_bg100_fg200_rep2000.cog')

# Create a spatstat window and ppp object.
ex_win <- intersect.owin(site_window,
  do.call(union.owin, lapply(unique(ex_path$x), function(x){
    return(owin(c(x - 3, x + 3), c(535421, 541130),
           unitname = c('foot', 'feet')))
  })))
ex_ppp <- ppp(ex_anom$x, ex_anom$y, window = ex_win)

# Create a ppp object the uses the full region as the window.
ex_ppp_full <- ppp(ex_anom$x, ex_anom$y, window = site_window)

par(mar = c(0, 0, 1, 0))
plot(ex_ppp, main = 'Observed Geomagnetic Anomalies', pch = '.', border = NA)
plot(site_window, add = TRUE)
@

I simulate data collection along parallel straight-line transects spaced 396
feet apart using a detector with a footprint six feet wide (so there is an
unobserved region 390 feet wide between each pair of adjacent transects). The
location of the first transect is selected randomly, and the locations of all
anomalies within three feet of a transect are
recorded~(Figure~\ref{fig:sim2000obs}). In total,
\Sexpr{sprintf('%.1f', area(ex_win) / 43560)} acres are observed, covering
\Sexpr{sprintf('%.1f\\%%', 100 * area(ex_win) / area(site_window))} of the
qegion of interest.

use \texttt{polynom()} not \texttt{poly()}!

<<polyfit1, warning = FALSE, fig.width = 6, fig.height = 8, fig.pos = 'p', fig.cap = ''>>=
# Create a quadrature scheme manually to ensure there are dummy points on
# each transect.
# TODO: Figure out weights.
ex_Q <- quad(data = ex_ppp,
             dummy = as.ppp(rbind(
               data.frame(vertices(Window(ex_ppp))),
               do.call(rbind, lapply(unique(ex_path$x), function(x){
                 return(data.frame(
                   x = x, y = seq(min(ex_path$y), max(ex_path$y),
                                  length.out = 128)
                 ))}))), W = Window(ex_ppp)))

ex_fit2 <- ppm(ex_Q, ~ polynom(scale(x, center = 1556870, scale = 7625),
                               scale(y, center = 535421, scale = 5709),
                                     2), correction = 'iso')
ex_fit3 <- ppm(ex_Q, ~ polynom(scale(x, center = 1556870, scale = 7625),
                               scale(y, center = 535421, scale = 5709),
                                     3), correction = 'iso')
ex_fit4 <- ppm(ex_Q, ~ polynom(scale(x, center = 1556870, scale = 7625),
                               scale(y, center = 535421, scale = 5709),
                                     4), correction = 'iso')
ex_fit5 <- ppm(ex_Q, ~ polynom(scale(x, center = 1556870, scale = 7625),
                               scale(y, center = 535421, scale = 5709),
                                     5), correction = 'iso')
ex_fit6 <- ppm(ex_Q, ~ polynom(scale(x, center = 1556870, scale = 7625),
                               scale(y, center = 535421, scale = 5709),
                                     6), correction = 'iso')
ex_fit7 <- ppm(ex_Q, ~ polynom(scale(x, center = 1556870, scale = 7625),
                               scale(y, center = 535421, scale = 5709),
                                     7), correction = 'iso')
ex_fit8 <- ppm(ex_Q, ~ polynom(scale(x, center = 1556870, scale = 7625),
                               scale(y, center = 535421, scale = 5709),
                                     8), correction = 'iso')
ex_fit9 <- ppm(ex_Q, ~ polynom(scale(x, center = 1556870, scale = 7625),
                               scale(y, center = 535421, scale = 5709),
                                     9), correction = 'iso')
ex_fit10 <- ppm(ex_Q, ~ polynom(scale(x, center = 1556870, scale = 7625),
                                scale(y, center = 535421, scale = 5709),
                                      10), correction = 'iso')
ex_fit11 <- ppm(ex_Q, ~ polynom(scale(x, center = 1556870, scale = 7625),
                                scale(y, center = 535421, scale = 5709),
                                      11), correction = 'iso')
ex_fit12 <- ppm(ex_Q, ~ polynom(scale(x, center = 1556870, scale = 7625),
                                scale(y, center = 535421, scale = 5709),
                                      12), correction = 'iso')

par(mfrow = c(4, 3), mar = c(1, 0, 1.2, 3), las = 2, cex = 1)

# Multiply predictions by 43560 to convert from anomalies per square foot
# to anomalies per acre.
plot(43560 * predict(ex_fit2, window = site_window), main = '2nd Degree')
plot(43560 * predict(ex_fit3, window = site_window), main = '3rd Degree')
plot(43560 * predict(ex_fit4, window = site_window), main = '4th Degree')
plot(43560 * predict(ex_fit5, window = site_window), main = '5th Degree')
plot(43560 * predict(ex_fit6, window = site_window), main = '6th Degree')
plot(43560 * predict(ex_fit7, window = site_window), main = '7th Degree')
plot(43560 * predict(ex_fit8, window = site_window), main = '8th Degree')
plot(43560 * predict(ex_fit9, window = site_window), main = '9th Degree')
plot(43560 * predict(ex_fit10, window = site_window), main = '10th Degree')
plot(43560 * predict(ex_fit11, window = site_window), main = '11th Degree')
plot(43560 * predict(ex_fit12, window = site_window), main = '12th Degree')
plot(43560 * density(ex_ppp_full) * area(ex_ppp_full) / ex_ppp_full$n,
     main = 'Kernel Density')
@

\texttt{plot} doesn't work, and \texttt{predict} doesn't work on the original
window. \texttt{predict} can't compute SEs.


\section{Discussion and Conclusion}

smallish SEs outside observed region because of faith in the model form --
need model checking

What geostatisticians and ordnance engineers will notice is that that SEs
don't reflect added uncertainty about unobserved regions.
Note kriging accounts for lack of model fit by autocorrelated errors --
prediction SEs increase far from obs because of lack of info about local
deviations from mean. For Poisson process, the events are independent given
the intensity so local variation is not helpful for prediction. Thus we are
heavily reliant on the correctness of the model form and the
representativeness of the observed subregion, so model checking is crucial.
If there is prior uncertainty about the form of the trend in a certain part of
the site, that region must be observed!

how much of the region do you need to observe to trust your model?


\appendix
\section{R Code Appendix}


\printbibliography

\end{document}
