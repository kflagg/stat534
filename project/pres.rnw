\documentclass[unknownkeysallowed]{beamer}
\usepackage{graphicx,epsfig,hyperref,booktabs,lmodern}

\usepackage[backend=bibtex,style=authoryear,citestyle=authoryear-comp]{biblatex}
\addbibresource{references.bib}

\beamertemplatenavigationsymbolsempty

\mode<presentation>
{
  \usetheme{bunsen}
  \setbeamercovered{transparent}
  \setbeamertemplate{items}[circle]
}

% set fonts
%\usepackage{fontspec}
%\setsansfont{Fontin Sans}
%\setbeamerfont{frametitle}{size=\LARGE,series=\bfseries}

% color definitions
\usepackage{color}
\definecolor{uipoppy}{RGB}{225, 64, 5}
\definecolor{uipaleblue}{RGB}{96,123,139}
\definecolor{uiblack}{RGB}{0, 0, 0}

% title slide definition
\title{Stat 534 Project: \\
Extrapolation from Poisson Process Intensity Surface Models}
\author{Kenny Flagg}
\institute[MSU]{Department of Mathematical Sciences \\
Montana State University}
\date{March 24, 2017}

\begin{document}

<<setup, echo = FALSE, message = FALSE, cache = FALSE>>=
library(knitr)
opts_chunk$set(echo = FALSE, comment = NA, message = FALSE,
               fig.path = 'pres_figure/', cache.path = 'pres_cache/',
               show.signif.stars = FALSE,
               fig.align = 'center', fig.width = 6.5, fig.height = 2.5,
               fig.pos = 'H', size = 'footnotesize', dev = 'pdf',
               dev.args = list(pointsize = 10))
knit_theme$set('print')

library(xtable)
options(xtable.table.placement = 'H', width = 80, scipen = 2,
        xtable.sanitize.rownames.function = function(x){return(x)},
        show.signif.stars = FALSE)
@

<<packages, message = FALSE, cache = FALSE>>=
library(spatstat)

set.seed(83534)
@

\setbeamertemplate{background}
{\includegraphics[width=\paperwidth,height=\paperheight]{beamer_title_page1}}
\setbeamertemplate{footline}[default]

\begin{frame}
	\vspace*{.5cm}
	\begin{center}
		\textcolor{white}{\huge Stat 534 Project: \\
Extrapolation from Poisson Process Intensity Surface Models \\}
		\vspace{.5cm}
		\textcolor{white}{\Large Kenny Flagg}\\
	\end{center}
\end{frame}

% Set the background for the rest of the slides.
% Insert infoline
\setbeamertemplate{background}
 {\includegraphics[width=\paperwidth,height=\paperheight]{msubg2}}
%\setbeamertemplate{footline}[bunsentheme]


\begin{frame}
\frametitle{Introduction}

\begin{itemize}

\item Goals

\begin{itemize}

\item Estimate inhomogeneous intensity surface from events in a subregion

\item Infer intensity across entire region

\end{itemize}

\item Applications

\begin{itemize}

\item Mapping where endangered species are located

\item Mapping geomagnetic anomalies prior to an unexploded ordnance (UXO)
remediation

\end{itemize}

\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Maximum Likelihood Intensity Surface Fitting}

\begin{itemize}

\item General point processes

\begin{itemize}

\item The theory is not too complicated but the computation is very difficult

\end{itemize}

\item Poisson processes

\begin{itemize}

\item Doable with numerical methods

\item Log-likelihood of Poisson with intensity \(\lambda(\mathbf{s})\) on
region \(D\) (note typos in \textcite{digglebook})
\begin{align*}
\ell(\lambda) &= \left\{-\mu + n\log(\mu) - \log(n!)\right\}
+ \sum_{i=1}^{n}\left\{\log\left(\lambda(\mathbf{s}_{i})\right)
- \log(\mu)\right\} \\
&= \sum_{i=1}^{n}\log\left(\lambda(\mathbf{s}_{i})\right)
-\int_{D}\lambda(\mathbf{s})d\mathbf{s} - \log(n!).
\end{align*}
where \(\mu=\int_{D}\lambda(\mathbf{s})d\mathbf{s}\)
\end{itemize}

\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Poisson Process Log-Linear Model}

\begin{itemize}

\item Assuming events are independent (conditional on the intensity function),
\begin{equation*}
\log(\lambda(\mathbf{s})) = \mathbf{x}(\mathbf{s})^{T}\boldsymbol{\beta}
\end{equation*}
where \(\mathbf{x}(\mathbf{s})^{T}\) is a row of predictors at location
\(\mathbf{s}\)

\item Predictors can include covariates, but they must be known across the
whole region

\item \textcite{bermanturner} use dummy points and quadrature to set up an
approximation as a weighted Poisson regression

\item Their method is implemented in \texttt{spatstat}'s \texttt{ppm}, with
\texttt{glm} from base R or \texttt{gam} from \texttt{mgcv} as the back-end,
using the \texttt{quasi} family

\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Simple Example}

<<hw4_sim>>=
simulate_win <- owin(c(0, 1), c(0, 1))
omit_win <- owin(c(0.6, 0.9), c(0.6, 0.9))
observe_win <- setminus.owin(simulate_win, omit_win)
predict_win <- owin(c(-0.5, 1.5), c(-0.5, 1.5))
hw4 <- rpoispp(function(x, y){exp(5 * x + 2 * y)}, win = simulate_win)
hw4_fit_s <- ppm(hw4[observe_win] ~ x + y, correction = 'isotropic')
hw4_pred_s <- predict(hw4_fit_s, window = predict_win, se = TRUE)
@

\begin{itemize}

\item True model

\begin{itemize}

\item Poisson process on the unit square

\item \(\displaystyle \log(\lambda(x,y)) = 5x + 2y\)

\end{itemize}

\item But we don't observe
\(\Sexpr{min(omit_win$xrange)} < x < \Sexpr{max(omit_win$xrange)},
\Sexpr{min(omit_win$yrange)} < y < \Sexpr{max(omit_win$yrange)}\)

\end{itemize}

<<hw4_plot>>=
par(mar = c(0, 0, 1, 0))
plot(hw4, main = 'Event Locations')
plot(omit_win, lty = 2, add = TRUE)
@
\end{frame}


\begin{frame}
\frametitle{Estimated Model}

\begin{itemize}

\item Fit the model
\(\displaystyle \log(\lambda(x,y)) = \beta_{0} + \beta_{1}x + \beta_{2}y\)

\end{itemize}

\begin{minipage}{0.5\textwidth}
<<hw4_fitcoefs_s, results = 'asis'>>=
hw4_coef_s <- summary(hw4_fit_s)$coefs.SE.CI[,c('Estimate', 'S.E.')]
rownames(hw4_coef_s) <- c('\\(\\widehat{\\beta}_{0}\\)',
                          '\\(\\widehat{\\beta}_{1}\\)',
                          '\\(\\widehat{\\beta}_{2}\\)')
xtable(hw4_coef_s)
@
\end{minipage}\begin{minipage}{0.5\textwidth}
<<hw4_dummy_s, fig.width = 3.25>>=
par(mar = c(0, 0, 1, 0))
plot(hw4_fit_s$Q, main = 'Data and Dummy Points')
@
\end{minipage}

\end{frame}


\begin{frame}
\frametitle{Extrapolate the Surface}

\begin{itemize}

\item Use the \texttt{predict} method

\item Specify a new window
\(\Sexpr{min(predict_win$xrange)} < x < \Sexpr{max(predict_win$xrange)},
\Sexpr{min(predict_win$yrange)} < y < \Sexpr{max(predict_win$yrange)}\)

\end{itemize}

<<hw4_resultplot>>=
par(mfrow = c(1, 2), mar = c(0, 0, 3, 0), cex = 1, las = 2)
plot(log(hw4_pred_s$estimate), zlim = c(-4.5, 10.5),
     main = expression(log(hat(lambda)(italic(list(x,y))))))
plot(observe_win, add = TRUE, border = '#ffffff80')
points(hw4[observe_win], pch = '.', col = 'white')
plot(log(hw4_pred_s$se), zlim = c(-4.5, 10.5),
     main = expression(log(SE(hat(lambda)(italic(list(x,y)))))))
plot(observe_win, add = TRUE, border = '#ffffff80')
points(hw4[observe_win], pch = '.', col = 'white')
@
\end{frame}

\begin{frame}
\frametitle{Where is the Uncertainty?}

\begin{itemize}

\item Relative standard error is lowest where the highest intensity was
observed

\end{itemize}

<<hw4_rse>>=
par(mar = c(0, 0, 1.5, 0), las = 2)
plot(hw4_pred_s$se/hw4_pred_s$estimate, zlim = c(0, 1),
     main = expression(SE(hat(lambda)(italic(list(x,y)))) /
                         hat(lambda)(italic(list(x,y)))))
plot(observe_win, add = TRUE, border = '#ffffff80')
points(hw4[observe_win], pch = '.', col = 'white')
@

\end{frame}


\begin{frame}
\frametitle{Simple UXO Site}

<<uxoexampledata>>=
site_window <- owin(poly = cbind(x = c(1564294, 1564495, 1556870, 1557126),
                                 y = c(535421, 541130, 541085, 535576)),
                    unitname = c('foot', 'feet'))
ex_anom <- read.csv('easy_sample_tTA2_p200_bg100_fg200_rep2000.anomaly')
ex_path <- read.csv('easy_sample_tTA2_p200_bg100_fg200_rep2000.cog')
ex_win <- intersect.owin(site_window,
  do.call(union.owin, lapply(unique(ex_path$x), function(x){
    return(owin(c(x - 3, x + 3), c(535421, 541130),
           unitname = c('foot', 'feet')))
  })))
ex_ppp <- ppp(ex_anom$x, ex_anom$y, window = ex_win)
ex_ppp_full <- ppp(ex_anom$x, ex_anom$y, window = site_window)
@

\begin{itemize}

\item \Sexpr{sprintf('%.2f', area(site_window) / 43560)} acre region (roughly
\Sexpr{prettyNum(c(-1, 1) %*% site_window$xrange, big.mark = ',')} ft by
\Sexpr{prettyNum(c(-1, 1) %*% site_window$yrange, big.mark = ',')} ft)

\item High density of geomagnetic anomalies around targets

\item Low density of background anomalies

\end{itemize}

<<truth, cache = TRUE>>=
gauss.elliptic <- function(x, y, mu.x = 0, mu.y = 0, s.a = 1, s.b = 1,
                           r = 0, maxrate = 1){
  rot <- zapsmall(matrix(c(cos(r), sin(r), -sin(r), cos(r)), nrow = 2))
  ab <- diag(c(s.a^2, s.b^2))
  sigma <- rot %*% ab %*% t(rot)
  siginv <- solve(sigma)
  mu <- matrix(c(mu.x, mu.y), nrow = 2)
  mat <- matrix(rbind(x, y), nrow = 2)
  return(maxrate * apply(mat, 2, function(vec){
      exp(-t(vec - mu) %*% siginv %*% (vec - mu) / 2)
    }))
}
x <- seq(1556880, 1564495, by = 20)
y <- seq(535431, 541130, by = 20)
intense.mat <- matrix(100, nrow = length(x), length(y))
for(i in seq_along(x)){
  for(j in seq_along(y)){
    intense.mat[i, j] <- 100 + gauss.elliptic(x[i], y[j],
        mu.x = 1558400, mu.y = 540000,
        s.a = 800 / (2 * qnorm(0.995)), s.b = 1200 / (2 * qnorm(0.995)),
        r = pi/6, maxrate = 200
      ) + gauss.elliptic(x[i], y[j],
        mu.x = 1562000, mu.y = 537000,
        s.a = 2000 / (2 * qnorm(0.995)), s.b = 900 / (2 * qnorm(0.995)),
        r = 0, maxrate = 200
      )
  }
}
intense.im <- im(t(intense.mat), x, y, unitname = c('foot', 'feet'))

par(mar = c(0, 0, 1, 3))
plot(intense.im, main = 'True Intensity Surface',
     border = NA, zlim = c(100, 300), las = 2)
plot(setminus.owin(owin(c(1556870, 1564495), c(535421, 541130),
                        unitname = c('foot', 'feet')),
     site_window), col = 'white', border = 'white', lwd = 2, add = TRUE)
mtext('Anomalies per Acre', 4, line = -1)
@

\end{frame}


\begin{frame}
\frametitle{Observed Events}

\begin{itemize}

\item Metal detectors record anomalies in six foot wide strips along parallel
transects with 396 feet between centerlines

\item Observed \Sexpr{sprintf('%.1f', area(ex_win) / 43560)} acres,
\Sexpr{sprintf('%.1f\\%%', 100 * area(ex_win) / area(site_window))}
of the site

\end{itemize}

<<sim2000obs>>=
par(mar = c(0, 0, 1, 0))
plot(ex_ppp, main = 'Observed Geomagnetic Anomalies', pch = '.', border = NA)
plot(site_window, add = TRUE)
@
\end{frame}


\begin{frame}
\frametitle{Trend Surface Models}

\begin{itemize}

\item Polynomial models
\begin{equation*}
\log(\lambda(x,y)) = \sum_{i=0}^{p}\sum_{j=0}^{p-i}\beta_{ij}x^{i}y^{j};
\qquad p = 2, 3, \dots, 12
\end{equation*}

\item Can approximate complicated surfaces

\item Expect two peaks, so even \(p \geq 4\) could work well

\item Rescaling \(x\) and \(y\) to mean 0 and variance 1 reduces numerical
instability for large \(p\)

\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Extrapolated Surfaces}

<<polyfit, warning = FALSE, cache = TRUE>>=
ex_fit1 <- ppm(ex_ppp ~ x + y)
ex_Q <- quad(data = ex_ppp, dummy = runifpoint(500000, Window(ex_ppp)))

ex_fit2 <- ppm(ex_Q, ~ polynom(scale(x), scale(y), 2), correction = 'iso')
ex_fit3 <- ppm(ex_Q, ~ polynom(scale(x), scale(y), 3), correction = 'iso')
ex_fit4 <- ppm(ex_Q, ~ polynom(scale(x), scale(y), 4), correction = 'iso')
ex_fit5 <- ppm(ex_Q, ~ polynom(scale(x), scale(y), 5), correction = 'iso')
ex_fit6 <- ppm(ex_Q, ~ polynom(scale(x), scale(y), 6), correction = 'iso')
ex_fit7 <- ppm(ex_Q, ~ polynom(scale(x), scale(y), 7), correction = 'iso')
ex_fit8 <- ppm(ex_Q, ~ polynom(scale(x), scale(y), 8), correction = 'iso')
ex_fit9 <- ppm(ex_Q, ~ polynom(scale(x), scale(y), 9), correction = 'iso')
ex_fit10 <- ppm(ex_Q, ~ polynom(scale(x), scale(y), 10), correction = 'iso')
ex_fit11 <- ppm(ex_Q, ~ polynom(scale(x), scale(y), 11), correction = 'iso')
ex_fit12 <- ppm(ex_Q, ~ polynom(scale(x), scale(y), 12), correction = 'iso')

ex_p2 <- predict(ex_fit2, window = site_window, se = TRUE)
ex_p3 <- predict(ex_fit3, window = site_window, se = TRUE)
ex_p4 <- predict(ex_fit4, window = site_window, se = TRUE)
ex_p5 <- predict(ex_fit5, window = site_window, se = TRUE)
ex_p6 <- predict(ex_fit6, window = site_window, se = TRUE)
ex_p7 <- predict(ex_fit7, window = site_window, se = TRUE)
ex_p8 <- predict(ex_fit8, window = site_window, se = TRUE)
ex_p9 <- predict(ex_fit9, window = site_window, se = TRUE)
ex_p10 <- predict(ex_fit10, window = site_window, se = TRUE)
ex_p11 <- predict(ex_fit11, window = site_window, se = TRUE)
ex_p12 <- predict(ex_fit12, window = site_window, se = TRUE)
@

<<resplots1>>=
par(mfrow = c(2, 3), mar = c(1, 0, 1.2, 3), las = 2, cex = 1)
plot(43560 * ex_p2$estimate, main = '2nd Degree Polynom')
plot(43560 * ex_p3$estimate, main = '3rd Degree Polynom')
plot(43560 * ex_p4$estimate, main = '4th Degree Polynom')
plot(43560 * ex_p5$estimate, main = '5th Degree Polynom')
plot(43560 * ex_p6$estimate, main = '6th Degree Polynom')
plot(43560 * ex_p7$estimate, main = '7th Degree Polynom')
@

\end{frame}


\begin{frame}
\frametitle{Extrapolated Surfaces}

<<resplots2>>=
par(mfrow = c(2, 3), mar = c(1, 0, 1.2, 3), las = 2, cex = 1)
plot(43560 * ex_p8$estimate, main = '8th Degree Polynom')
plot(43560 * ex_p9$estimate, main = '9th Degree Polynom')
plot(43560 * ex_p10$estimate, main = '10th Degree Polynom')
plot(43560 * ex_p11$estimate, main = '11th Degree Polynom')
plot(43560 * ex_p12$estimate, main = '12th Degree Polynom')
plot(43560 * density(ex_ppp_full) * area(ex_ppp_full) / area(ex_ppp),
     main = 'Kernel Density')
@

\end{frame}


\begin{frame}
\frametitle{Standard Errors}

<<seplots>>=
par(mfrow = c(2, 3), mar = c(1, 0, 1.2, 3), las = 2, cex = 1)
plot(log(43560 * ex_p2$se), main = '2nd Degree log(SE)')
plot(log(43560 * ex_p4$se), main = '4th Degree log(SE)')
plot(log(43560 * ex_p6$se), main = '6th Degree log(SE)')
plot(log(43560 * ex_p8$se), main = '8th Degree log(SE)')
plot(log(43560 * ex_p10$se), main = '10th Degree log(SE)')
plot(log(43560 * ex_p12$se), main = '12th Degree log(SE)')
@

\end{frame}


\begin{frame}
\frametitle{Problems with Implementation}

\vspace{-11pt}
\begin{itemize}

\item By default, \texttt{ppm} places dummy points on a grid across a bounding
box

\begin{itemize}

\item Only \Sexpr{prettyNum(ex_fit1$Q$dummy$n, big.mark = ',')} points on two
transects are kept

\end{itemize}

\item I draw \Sexpr{prettyNum(ex_Q$dummy$n, big.mark = ',')} points from a
uniform distribution on the observed region

\item \textcite{wartonshepherd} recommend using enough dummy points that the
maximized log-likelihood converges

\end{itemize}

<<dummy>>=
par(mfrow = c(1, 2), mar = c(1, 0, 1, 0), cex = 1)
plot(ex_fit1$Q$dummy, pch = '.', border = NA,
     main = 'Default Dummy Points')
plot(site_window, add = TRUE)
plot(ex_Q$dummy, pch = '.', border = NA,
     main = 'Manual Dummy Points')
plot(site_window, add = TRUE)
@

\end{frame}


\begin{frame}
\frametitle{Problems with Implementation}

\begin{itemize}

\item For \(p \geq 18\), \texttt{ppm} cannot compute SEs because the
``Fisher information matrix is singular'' --- 190 coefficients

\item The \texttt{plot} method gives an error about infinite values

\item When the window isn't specified, the \texttt{predict} method's default
grid misses all but two transects

\item The predict method does not work with spline smoothers

\item The magnitudes of the predictions are much too large

\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Conclusion}

\begin{itemize}

\item Polynomial surfaces are flexible but require much faith in the model

\item How to check these models?

\item How much of the region must be observed?

\item Can implement with existing R packages but not easily

\item What scale are the prediction SEs on?

\end{itemize}

\end{frame}


\begin{frame}[allowframebreaks]
\frametitle{References}

\vspace{-0.5cm}{\tiny
\printbibliography
}
\end{frame}

\end{document}
